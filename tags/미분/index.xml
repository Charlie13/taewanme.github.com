<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>미분 on taewan.kim 블로그</title>
    <link>http://taewan.kim/tags/%EB%AF%B8%EB%B6%84/</link>
    <description>Recent content in 미분 on taewan.kim 블로그</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Sep 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="http://taewan.kim/tags/%EB%AF%B8%EB%B6%84/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>tanh 미분 정리</title>
      <link>http://taewan.kim/post/tanh_diff/</link>
      <pubDate>Tue, 19 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/post/tanh_diff/</guid>
      <description>Hyperbolic Tangent(tanh) 함수는 Sigmoid의 대체제로 사용될 수 있는 활성화 함수입니다. Hyperbolic Tangent(tanh)는 Sigmoid와 매우 유사합니다. 실제로, Hyperbolic Tangent 함수는 확장 된 시그모이드 함수입니다. tanh와 Sigmoid의 차이점은 Sigmoid의 출력 범위가 0에서 1 사이인 반면 tanh와 출력 범위는 -1에서 1사이라는 점입니다. Sigmoid와 비교하여 tanh와는 출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습하는 특성이 있습니다.
Sigmoid와 비교하여 중심점이 0이고 범위가 기울기 넓은 차이점이 있지만 Sigmoid의 치명적인 단점인 Vanishing gradient problem 문제를 그대로 갖고 있습니다.</description>
    </item>
    
    <item>
      <title>Sigmoid 함수 미분 정리</title>
      <link>http://taewan.kim/post/sigmoid_diff/</link>
      <pubDate>Mon, 18 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/post/sigmoid_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
    <item>
      <title>선형회귀 MSE 오차함수 미분 및 코드 구현</title>
      <link>http://taewan.kim/post/cost_function_derivation/</link>
      <pubDate>Tue, 09 Aug 2016 16:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/post/cost_function_derivation/</guid>
      <description>지도학습의 선형회귀 모델은 비용 함수로 MSE(Mean squared error, 평균 제곱 오차) 사용합니다. MSE를 사용하여 가장 간단한 선형회귀 모델을 학습시키는 알고리즘을 구현해 보겠습니다.
이 문서에서는 여러 수식을 사용합니다. 수식에서 스칼라, 벡터, 행렬을 다음과 같은 표기법을 사용할 것입니다.
 $w$: 스칼라, 소문자 표기는 스칼라를 의미합니다. 예제에서는 가중치 1개를 의미합니다.
 $\boldsymbol{w}$: 벡터, 소문자 볼츠체는 벡트를 이미합니다. $W$: 행렬, 대문자는 행렬(Matrix)을 의미합니다.  본문에서 $\theta$와 $\boldsymbol{w}$는 모두 가중치 벡터를 의미합니다.
1. 지도학습이란? 지도학습의 데이터는 사례와 라벨로 구성됩니다.</description>
    </item>
    
  </channel>
</rss>