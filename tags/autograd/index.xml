<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>autograd on taewan.kim 블로그</title>
    <link>http://taewan.kim/tags/autograd/</link>
    <description>Recent content in autograd on taewan.kim 블로그</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Mar 2018 03:05:14 +0900</lastBuildDate>
    
	<atom:link href="http://taewan.kim/tags/autograd/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Neural Networks</title>
      <link>http://taewan.kim/trans/pytorch/tutorial/blits/03_neural_networks/</link>
      <pubDate>Tue, 27 Mar 2018 03:05:14 +0900</pubDate>
      
      <guid>http://taewan.kim/trans/pytorch/tutorial/blits/03_neural_networks/</guid>
      <description>ML 문서 번역 &amp;gt; PyTorch Tutorial &amp;gt; PyTorch와 함께하는 딥러닝: 60분 리뷰 &amp;gt; 신경망
  원문: http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html 원문 제목: Neural Networks   torch.nn 패키지를 사용하여 신경망을 만들 수 있습니다.
지금까지 autograd에 대하여 살펴보았습니다. nn 패지지는 autograd를 사용하여 모델을 정의하고 미분합니다. nn.Module은 여러 레이어와 forward(input) 메서드를 포함합니다. 이 forward 메서드는 output을 반환합니다.
다음 이미지는 디지틀 사진을 분류하는 신경망입니다.
convnet1
위 신경망은 단순한 feed-forward 네트워크입니다. 이 신경망은 입력된 데이터를 순차적으로 여러 레이어에 데이터를 공급합니다.</description>
    </item>
    
    <item>
      <title>Autograd: 미분 자동화</title>
      <link>http://taewan.kim/torchtrans/tutorial/blits/autograd/</link>
      <pubDate>Sun, 18 Mar 2018 03:05:14 +0900</pubDate>
      
      <guid>http://taewan.kim/torchtrans/tutorial/blits/autograd/</guid>
      <description>원문: http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html 원문 제목: Autograd: automatic differentiation?  PyTorch의 모든 신경 네트워크의 중심에는 autograd 패키지가 있습니다. 먼저 autograd에 대하여 간략히 살펴 보겠습니다. 그러나서 첫 번째 신경망을 훈련해 볼 것 입니다.
몇 가지 예로 좀 더 간단한 용어를 살펴보겠습니다. ## Variable ## Gradients ### NumPy 배열을 Torch 텐서로 변환 Python import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b)
 &amp;gt; 출력:  [ 2.</description>
    </item>
    
    <item>
      <title>Autograd: 미분 자동화</title>
      <link>http://taewan.kim/trans/pytorch/tutorial/blits/02_autograd/</link>
      <pubDate>Tue, 27 Feb 2018 03:05:14 +0900</pubDate>
      
      <guid>http://taewan.kim/trans/pytorch/tutorial/blits/02_autograd/</guid>
      <description>ML 문서 번역 &amp;gt; PyTorch Tutorial &amp;gt; PyTorch와 함께하는 딥러닝: 60분 리뷰 &amp;gt; Autograd: 미분 자동화
  원문: http://http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html 원문 제목: Autograd: automatic differentiation   PyTorch으로 만든 모든 신경망의 중심에는 autograd 패키지가 있습니다. 먼저 autograd 패키지를 간략히 살펴보겠습니다. 그리구 다음 문서에서 첫 번째 신경망을 훈련해 보겠습니다.
[^1]: 역자주 - Weight와 Bias가 업데이트 되는 단위 Mini-Batch를 의미합니다. 좀 더 간단한 용어로 몇 가지 예를 들어 보겠습니다. ## Variable autograd.</description>
    </item>
    
  </channel>
</rss>