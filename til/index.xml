<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tils on taewan.kim 블로그</title>
    <link>http://taewan.kim/til/</link>
    <description>Recent content in Tils on taewan.kim 블로그</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Aug 2018 20:28:14 +0900</lastBuildDate>
    
	<atom:link href="http://taewan.kim/til/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[til]처음시작하는 머신러닝 11장</title>
      <link>http://taewan.kim/til/first_ml_05/</link>
      <pubDate>Sat, 18 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/first_ml_05/</guid>
      <description> 11장 문서 분석  문서 분류 시스템 토픽 모델 시스템 품사 분석 시스템 고유명사 태깅 시스템 단어 임베딩 학습  </description>
    </item>
    
    <item>
      <title>[til]처음시작하는 머신러닝 6-9장</title>
      <link>http://taewan.kim/til/first_ml_04/</link>
      <pubDate>Sat, 18 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/first_ml_04/</guid>
      <description>7장 추천 유사도  Jaccard similarity coefficient: 두 집합의 원소의 유사도  개념  두 집합 A, B $\frac{|A \cap B|}{|A \cup B|} $ 두 집합의 교집합 원소 수와 합집합 원소 수의 비율  특징  희소 벡터로 구성된 데이터에 대한 집단 비교에 유용함   Cosine Similarity: 방향  $sim(X, Y) = \frac{X \cdot Y}{||X||\ ||Y||}$  X, Y 벡터   Edit Distance: 작업 연산 수  편집 연산의 수  Insert(삽입) Delete(삭제) Substitution(대체) transposition(전치)  예  슈퍼맨 1, 슈퍼맨 2: 2 (1삭제, 2삽입)    Recommendation 내용 기반 추천  장점  사용자 정보 없이 추천 가능 이해하기 쉬움  단점  독특한 아이템 추천이 어려움 신규 사용자 추천 어려움   CF 추천  사용자 기반 협업 필터링  다른 사람의 구매 이력을 이용하여 추천  상품 기반 협업 필터링</description>
    </item>
    
    <item>
      <title>[til]처음시작하는 머신러닝 4-5장</title>
      <link>http://taewan.kim/til/first_ml_03/</link>
      <pubDate>Fri, 17 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/first_ml_03/</guid>
      <description>4. 군집화 Euclidean Distance $$ d(x, y) = \sqrt{\sum^n_{i=1}(x_i-y_i)^2} = ||X-Y|| $$
$$ Squared Euclidean Distance = \sum^n_{i=1}(x_i-y_i)^2=||X-Y||^2_2 $$
군집화 유형  중심기반 군집화  k-means clustering k-medians clustering k-modes clustering  계층적 군집화 밀도기반 군집화  중심기반 군집화  K개의 임의의 포인트 선정 각 데이터와 K개 포인트의 거리 계산 각 데이터를 K개의 포인트에 할당 K개 포인트를 중심점으로 이동 2-4를 반복  계층적 군집화  최상의 클러스터: 모든 데이터 포함 최하위 클러스터: 1개의 데이터 포함 클러스터 방식  하향식 분할적 클러스터화  전체를 1개의 클러스터로 지정 중심점 지정 중심점에서 가장 먼 데이터 확인 중심점과 먼 거리 데이터를 기준으로 거리 계산 중심점과 먼거리에 데이터 할당 클러스터 별로 2-5반복  상향식 집괴적 클러스터화  1개의 1개의 클러스터로 지정 거리가 가까운 2개를 뭉쳐 클러스터화 거리가 가까운 2개 클러스터 뭉침  가까운 거리 기준 먼거리 기준 평균거리 기준  1개가 남을때 까지 반복    밀도 기반 군집화  유형  평균 이동 군집화 DBSCAN: Density-based spatial clustering of applications with noise  용어  core point: 반경 Epsilon안에 일정 개수 이상의 데이터가 존재하는 데이터 border point: 중심 포인트보다 적지만, 중심 포인트로 부터 반경 Epsilon안에 존재하는 데이터 noise point: core point도 border point도 아닌 데이터     유사도 계산  타입  Minkoski Distance: 벡터 공간의 두 점간 거리 Mahalanobis distance: 점간 분포를 고려한 거리   minkowski distance  $$ d(X, Y) = \sqrt[p]{\sum^m_{i=1}|x_i-y_i|^p} $$</description>
    </item>
    
    <item>
      <title>[til]처음시작하는 머신러닝 2-3장</title>
      <link>http://taewan.kim/til/first_ml_02/</link>
      <pubDate>Wed, 15 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/first_ml_02/</guid>
      <description>2장. 주요 개념  Topic  Model Loss Function Optimization Model Evaluation   Model  모델이란 데이터에 대한 가정(Hypothesis)의 총합
 통계학에서는 Hypothesis를 Belief라고 함  단순(간단) 모델
 데이터의 단순성을 가정 이해하기 쉬운 결과 학습 용이 복잡한 데이터를 학습하기 어려움(표현력 제약)  복잡한 모델
 가정이 없음 이해하기 어려운 결과 학습이 복잡 새로운 데이터에 대한 성능이 떨어짐  결정 트리
 트리의 분기마다 1가지 조건으로 분기 마지막 리프노드에 결과 하당   구조적 모델  순차모델  Sequence Model  RNN: Recurrent neural net, 순환신경망  수식: $h_t = w_0 + w1h(t-1) + w_2x_t $  CRF: Conditional Random Field, 조건부 랜덤 필드   그래프모델  Markov Random Field  문서의 문법구조 이미지 픽셀 사이의 관계를 그래프로 표현    좋은 모델?</description>
    </item>
    
    <item>
      <title>[til]처음시작하는 머신러닝 1장</title>
      <link>http://taewan.kim/til/first_ml_01/</link>
      <pubDate>Mon, 13 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/first_ml_01/</guid>
      <description>머신러닝 정의  머신러닝이란? 데이터를 이용하여 명시적으로 정의하지 않은 패턴을 컴퓨터로 학습하여 결과를 만들어내는 학문 분야
 1959, 아서 사무엘(Arthur Lee Samuel)   머신러닝 구성 요소  Data Pattern Recognition Computing  머시러닝 관련 학문  수학 - 행렬 - 선형대수: 행렬분해 - 확률: 조건부 확률 통계학: 데이터에서 패턴을 찾아내는 학문 - 정규 분포, 가우스분포, 상관관계 컴퓨터 공학  고집적 연산 병렬 연산 프로그래밍   과거와 현재  1950년: 얼런튜닝의 튜링테스트 제안 1957년: Percetron 제안 1990 - 2010: 통계적 머신러닝 2010 ~ : 빅데이터 2013 ~ : 딥러닝 - GPU 발전 - 데이터 증가 - 알고리즘 발전</description>
    </item>
    
    <item>
      <title>머신러닝을 위한 기초 수학</title>
      <link>http://taewan.kim/til/basic_math/</link>
      <pubDate>Wed, 08 Aug 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_math/</guid>
      <description>시그마, 파이, 미분, 편미분, 합성함수, 백터 및 행렬, 기하벡터, 지수, 대수에 대하여 요점 정리합니다.
Sigma &amp;amp; Phi  1 부터 100까지 합산  $$ \sum_{i=1}^{100}i $$
 몇시적으로 마지막 값을 모르는 합산  $$ \sum_{i=1}^{n}i $$
 집합 합산  $$ G = {2, 3, 6, 8, 10} \\
\sum_{g \in G}g $$
 모든 것을 곱함 (1~100)  $$ 1 \cdot 2 \cdot 3 \cdot 4 \cdot \cdot \cdot 99 \cdot 100 \\</description>
    </item>
    
    <item>
      <title>비용 함수 MSE를 미분하여 경사하강법 유도</title>
      <link>http://taewan.kim/til/mse_gd/</link>
      <pubDate>Wed, 08 Aug 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/mse_gd/</guid>
      <description>2장. 지도학습  지도학습  &amp;ldquo;Input/Label&amp;rdquo;로 구성된 사례 데이터를 이용하여 ML 모델을 학습 최종 목표: 새로운 데이터를 정확하게 에측하는 것 유형: 회귀 &amp;amp; 분류   2.1 분류와 회귀  그림 1: 지도학습 요약    2.2 일반화, 과대적합, 과소적합  Generalization:  일반화 새로운 데이터에 정확한 예측을 제공하는 모델의 역량 모델이 복잡해 지면 학습셋에만 정확한 예측 제공 학습데이터에만 정확도를 보이는 상태를 과대적합(Overfitting) 되었다고 함 데이터의 노이즈가지 학습한 상태를 의미  Overfitting  학습셋에는 좋은 정확도를 보이지만, 새로운 데이터에 대해서 정확도가 상당히 떨어짐 과도하게 복잡한 모델 사용 모든 데이터 암기 학습셋의 노이즈까지 익힌 상태  Underfitting  학습이 아직 부족한 상태</description>
    </item>
    
    <item>
      <title>Classification 성능 matrix</title>
      <link>http://taewan.kim/til/matrics_in_classification/</link>
      <pubDate>Tue, 07 Aug 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/matrics_in_classification/</guid>
      <description> 분류 평가 기준  그림 1: Accuracy: 정확도     정확도: 전체 데이터 중에서 정확하게 분류한 비율   그림 2: Precision: 정밀도     정밀도: 양성의 예측 중에서 진짜로 양성인 비율  양성의 품질을 중요시 하는 경향    그림 3: Recall     재현율: 실제 양성 중에서 양성으로 분류한 비율  원본의 상태를 중시하는 경향   ROC Curve  그림 4: Recall &amp;amp; FPR     진짜 양성 비율: 양성관측한 것이 양성 가짜 음성 비율: 양성으로 예측, 음성  </description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#8: 확률 변수, 확률 분포, 이산확률 분포 </title>
      <link>http://taewan.kim/til/basic_statistics_09/</link>
      <pubDate>Mon, 06 Aug 2018 20:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_09/</guid>
      <description>01. 확률 변수, 확률 분포 확률변수, 확률 분포는 함수다.
  Sample Space는 시행에서 얻어지는 모든 결과의 집합. Sample Space의 모든 원소를 실수로 대응하는 함수: 확률 변수 확률 변수로 얻어진 실수를 확률값으로 변화하는 함수: 확률 분포 확률 변수와 확률 분포를 이용하여 시행의 결과를 실수로 변환할 수 있고, 발생 확률로 변환할 수 있다.   그림 1: 확률 변수와 확률 분포는 함수다     최종 확률분포표로 제시  02.</description>
    </item>
    
    <item>
      <title>가우스가 들려주는 근삿값과 오차 이야기</title>
      <link>http://taewan.kim/til/gauss_distribution/</link>
      <pubDate>Sun, 05 Aug 2018 10:50:36 +0900</pubDate>
      
      <guid>http://taewan.kim/til/gauss_distribution/</guid>
      <description>오차의 역사 천문학에서는 특정 관측이 다양한 관측치로 측정되는 문제가 있습니다. 이 중에서 어떤 것이 정확한 관측인지를 알기 위해서 오차를 없애는 방법을 학문적으로 연구하기 시작했습니다.
갈릴레오 갈렐리이는 참값(Truth Value)에는 많은 관측치가 존재하지만 먼 관측치는 드물다. 관측치의 빈도를 그래프로 그리면 종 모양이 된다고 했고, 이 개념은 후에 가오스 분포로 증명되었습니다.
 그림 1: 관측치 빈도 그래프: 종모양 =&amp;gt; 가우스 정규분포의 시작점     근사값: 참값은 아니지만 참값에 가까운 값 오차: 근사값 - 참값 오차의 한계: 근사값에 대한 오차의 절대값이 어떤 값 이하일때, 그 값을 근사값에 대한 오차의 한계하고 함  $오차의 한계 \geq |근사값-참값|$ 오차에 대한 최댓값 $최소자리수 X \frac{1}{2}$  참값의 범위: 근사값에 오차를 더하거나 뺀 것  $(근사값) - (오차의 한계) \leq 참값 \leq (근사값) + (오차의 한계)$  유효숫자: 근사값을 나타내는 숫자 중에서 믿을 수 있는 숫자  반올림한 근사값이 1,250일때 유효 숫자는 1, 2, 5 측정한 값의 경우 실제로 눈금을 읽어서 얻은 숫자   유효 숫자 표현  유효숫자가 a인 근사값 표현  $a * 10^n (1 \leq a \lt 10, n은 양의 정수)$ $a * \frac{1}{10^n} (1 \leq a \lt 10, n은 양의 정수)$   느낌 참값은 신의 값이라는 개념에서 시작하여, 정확한 값을 찾기 위한 노력이 정규분포로 발전하였다.</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#8: 사건의 독립과 종속 </title>
      <link>http://taewan.kim/til/basic_statistics_08/</link>
      <pubDate>Fri, 03 Aug 2018 23:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_08/</guid>
      <description>01. 사건의 독립과 종속  예제  10개의 제비뽑기가 있음, 당첨제베는 2개, 철수와 영희 순서로 제비뽑기 진행 복원추출: 제비를 뽑으면 다시 주머니에 추가 비복원추출: 뽑힌 제비는 재사용하지 않음, 제거     복원 추출      철수의 확률 $\Longrightarrow$ 영희가 당청될 확률     철수의 당첨 사건 $\frac{2}{10}$ $\Longrightarrow$ $\frac{2}{10}$   철수의 비당첨 사건 $\frac{8}{10}$ $\Longrightarrow$ $\frac{2}{10}$      철수의 당첨 여부는 영희의 당첨 사건 확률은 변하지 않음 철수의 당첨 사건은 영희의 당첨 사건에 영향을 미치지 않음    비복원 추출      철수의 확률 $\Longrightarrow$ 영희가 당청될 확률     철수의 당첨 사건 $\frac{2}{10}$ $\Longrightarrow$ $\frac{1}{9}$   철수의 비당첨 사건 $\frac{8}{10}$ $\Longrightarrow$ $\frac{2}{10}$      철수의 당첨 여부에 따라서 영희의 당첨 활률이 변함 철수의 당첨 사건은 영희의 당첨 사건에 영향을 미침  독립과 종속  종속: 사건이 2개 있을 때, 한 사건이 다른 사건에 영향을 주는 것  두 사건이 종속되었다라고 표현 비복원  독립: 사건이 2개 있을 때, 한 사건이 다른 사건에 영향을 주지 않는 것  두 사건이 됙립되었다라고 표현 복원   수식으로 살펴본 종속  $$ P(B|A) \neq P(B|A^c) $$</description>
    </item>
    
    <item>
      <title>&#39;파이썬 라이브러리를 활용한 머신러닝&#39; 2장. 지도학습</title>
      <link>http://taewan.kim/til/plwml-02/</link>
      <pubDate>Fri, 03 Aug 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/plwml-02/</guid>
      <description>2장. 지도학습  지도학습  &amp;ldquo;Input/Label&amp;rdquo;로 구성된 사례 데이터를 이용하여 ML 모델을 학습 최종 목표: 새로운 데이터를 정확하게 에측하는 것 유형: 회귀 &amp;amp; 분류   2.1 분류와 회귀  그림 1: 지도학습 요약    2.2 일반화, 과대적합, 과소적합  Generalization:  일반화 새로운 데이터에 정확한 예측을 제공하는 모델의 역량 모델이 복잡해 지면 학습셋에만 정확한 예측 제공 학습데이터에만 정확도를 보이는 상태를 과대적합(Overfitting) 되었다고 함 데이터의 노이즈가지 학습한 상태를 의미  Overfitting  학습셋에는 좋은 정확도를 보이지만, 새로운 데이터에 대해서 정확도가 상당히 떨어짐 과도하게 복잡한 모델 사용 모든 데이터 암기 학습셋의 노이즈까지 익힌 상태  Underfitting  학습이 아직 부족한 상태</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#7: 확률 개념 </title>
      <link>http://taewan.kim/til/basic_statistics_07/</link>
      <pubDate>Thu, 02 Aug 2018 23:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_07/</guid>
      <description>01. 확률의 개념 용어정리  Trial: 시행이란 동일한 조건에서 여러번 반복할 수 있고 그 결과가 우연에 의해서 결정되는 관찰이나 실험  예제: 주사위 던지기  Sample Space: 표본 공간  Trial(시행)의 결과들의 집합 예제 - 동전을 던지는 실험에서 표본 공간은 {앞면, 뒷면} - 6면 주사위를 던지는 실험에서 표본 공간은 {1, 2, 3, 4, 5, 6}  Event: 사건  표본 공간의 부분 집합  fundamental event: 근원 사건  원소의 갯수가 한 개인 사건 표본 공간이 한 원소로 이루어 집합  합사건: 두 사건 A와 B의 합집합으로 표현할 수 있는 사건  $A \cup B$ 예제: 4의 약수가 나오거나 또는 홀수가 나오는 사건 {1,2, 3, 4}  곱사건: 두 사건 A와 B의 교집합으로 표현할 수 있는 사건  $A \cap B$ 예제: 4의 약수 그리고 혹수 =&amp;gt; {1}  여사건: 여집합으로 표현되는 사건  $A^c$ 예제: 4의 약수가 나오지 않는 사건  배반사건: A와 B 사전의 교집합니 공집합인 사건  $A \cap B= \emptyset$   수학적 확률  수학적 확률은 근원 사건이 일어날 가능성이 모두 같을 때, 수학적 확률을 계산할 수 있다.</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#6: 이항정리 </title>
      <link>http://taewan.kim/til/basic_statistics_06/</link>
      <pubDate>Thu, 02 Aug 2018 11:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_06/</guid>
      <description>01. 이항정리  이항정리는?
 $(a+b)^n$이 전개되는 방식을 정리 항이 두 개인 거듭제곱의 전개 방식을 소개  $(a+b)(a+b)=a^2+ab+ba+b^2$
 $(a+b)(a+b)(a+b)$ : 8개의 항이 존재함 (2X2X2)
 aaa aab aba baa abb bab bba bbb $a^3+3a^2+3ab^2+b^3$ = $a^3$: $_3C_0$=&amp;gt; 3개 중에서 b를 한개도 뽑지 않는 경우의 수 = $3a^2b$: $_3C_1$=&amp;gt; 3개 중에서 b를 한 개 뽑을 경우의 수 = $3ab^2$: $_3C_2$=&amp;gt; 3개 중에서 b를 두 개 뽑을 경우의 수 = #b^3$: $_3C_3$=&amp;gt; 3개 중에서 b를 세 개 뽑을 경우의 수  $(a+b)(a+b)(a+b)(a+b)$ : 8개의 항이 존재함 (2X2X2)</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#5: 자연수 분할&amp;집합분할 </title>
      <link>http://taewan.kim/til/basic_statistics_05/</link>
      <pubDate>Wed, 01 Aug 2018 23:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_05/</guid>
      <description>01. 자연수 분할  생각해 볼 거리1
 5를 2개의 자연수 합으로 표현하는 방법  {1, 4}, {2, 3}, {3, 2}, {1, 4} : 순서를 고려할 필요 없음 2개  5를 3개의 자연수 합으로 표현  {3, 1, 1}, {2, 2, 1}   n = n_1+n_2+n_3+&amp;hellip;&amp;hellip;+n_k
 n을 k개의 수로 분할할 수 있음 n&amp;gt;=n_1&amp;gt;=n_2&amp;gt;=n_3&amp;gt;=&amp;hellip;..&amp;gt;=n_k 1 &amp;lt;= k &amp;lt;= n example  5 k=1 ==&amp;gt; {5} k=5 ==&amp;gt; {1,1,1,1,1}   자연수 n을 n보다 작거나 같은 k개의 자연수의 합으로 순서를 고려하지 않고 표현한 것은 자연수의 분할 이라고 한다.</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#4: 조합 </title>
      <link>http://taewan.kim/til/basic_statistics_04/</link>
      <pubDate>Mon, 30 Jul 2018 23:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_04/</guid>
      <description>1. 조합  순열(Permutation): 순서있게 나열하는 방법의 수  $_nP_r$: n개중에서 r개를 뽑아서 순서 있게 나열한 수 $\frac{n!}{(n-r)!}$  조합(Combination): 순서를 고려하지 않고 선택하는 방법의 수  순열을 순서를 고려한 경우의 수임 조합은 순서를 고려하지 않음 순열의 객수에 선택한 갯수의 경우의 수로 나워서 순열의 중복을 제거 순열의 순서를 제거 =&amp;gt; $_nC_r = \frac{_nP_r}{r!}$    예제  ABCDE 중에서 2개를 뽑아서 순서있게 나열하는 것  $_5P_2$  ABCDE 중에서 2개를 조합  순서를 고려하지 않음 선택만 함 5개 중에서 2개를 선택하는 것 $_5C_2$ = $\frac{_5P_2}{2!</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#3: 중복순열 </title>
      <link>http://taewan.kim/til/basic_statistics_03/</link>
      <pubDate>Sun, 29 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_03/</guid>
      <description>1. 같은것이 존재하는 순열 같은 것이 있는 순열  ABCD 나열: 4! AACD의 나열  ABCD에서 B를 A로 치환한것 ABCD와 BACD는 AACD가 됨 2가지 경우의 수는 1개가 됨 두 가지가 같은 것이므로 제외: 4!/2! = 12  AAAD의 나열  ABCD의 ABC를 A로 치환 ABCD, ACBD, BACD, BCAD, CABD, CBAD는 AAAD가 됨 6개가 1개로 됨 4!/3!=4  AABB이 나열  ABCD에서 C=A로 D=B로 치환 4!/(2!2!)= 6   일부의 순서가 결정된 순열  문제1: ABCDE가 존재한다.</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#2: 원순열 </title>
      <link>http://taewan.kim/til/basic_statistics_02/</link>
      <pubDate>Sat, 28 Jul 2018 01:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_02/</guid>
      <description>1. 원순열  순열: 일렬로 순서있게 나열하는 방법, 처음과 끝이 존재 원순열: 처음과 끝이 없는 상태로 순서있게 나열하는 방법, 원위에 나열하는 개념  처음과 끝이 없다 한방향으로 요소의 나열 순서가 같으면 같은 것으로 간주    처음과 끝이 없는 순열에 처음과 끝을 만드는 방법  첫번째 요소를 고정하여 기준으로 처음과 끝을 만듦 첫번째 요소는 모든 요소가 위치할 수 있음 전체 순열의 수에서 첫 번째 요소에 올 수 있는 요소 수를 나누면 원수열의 갯수가 됨 첫번째 요소는 기준을 정하는 역할을 담당할 뿐 경우의 수에 미치는 영향이 없어짐    $$ 원순열= /frac{n!</description>
    </item>
    
    <item>
      <title>[til]기초 확률&amp;통계#1: 경우의수 &amp; 순열 </title>
      <link>http://taewan.kim/til/basic_statistics_01/</link>
      <pubDate>Fri, 27 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/basic_statistics_01/</guid>
      <description>1. 경우의 수  합의 법칙: 동시에 일어나지 않는 사건 (각 사건의 경우의 수 합) 곡의 법칙: 동시에 일어나는 혹은 연달아 일어나는 두 사건(가 사건의 경우의 수 곱)  Case1 - 합의 법칙 사례  문제: 1-10 카드 10장 중에서 2장을 선택하여, 두 합이 7의 배수가 되는 경우의 수   풀이  두 장의 카드 합의 크기 범위: $3&amp;lt;= A+B &amp;lt;= 19$ 가능한 7의 배수: {7, 14} 두 카드를 선택할 때 순서는 중요하지 않음 두 카드의 합 7: (1, 6) (2, 5) (3, 4) 두 카드의 합 14: (4, 10) (5, 9) (6, 8) 7이 되는 사건과 14가 되는 사건은 동시에 발생할 수 없음 합의 법칙 적용 경우의 수 = 두 카드의 합 7의 경우의 수 + 두 카드의 합 14의 경우의 수 = 6   Case2 - 합의 법칙 사례  문제: 자연수 x와 y가 있다.</description>
    </item>
    
    <item>
      <title>[Handson_ML]ch02: ML 프로젝트 처음부터 끝까지</title>
      <link>http://taewan.kim/til/handson_ml_ch02/</link>
      <pubDate>Thu, 26 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/handson_ml_ch02/</guid>
      <description>머신러닝 프로제트 절차
 머신러닝 프로제트 절차
 문제 정의 데이터 수집 Data Discovery: 탐색과 시각화 for Insight 데이터 전처리 모델 선택 및 훈련 모델 미세 조정 해법(Solution) 제시 시스템 구축 및 모니터링, 유지보수   Dataset  주요 데이터 공개 사이트
 UC 얼바인: http://archive.ics.uci.edu/ml Kaggle: http://www.kaggle.com/datasets AWS: http://aws.amazon.com/ko/datasets meta potal site  http://dataportals.org http://opendatamonitor.eu http://quandl.com  인기 데이터셋 목록  List of datasets for machine learning research in wikipedia https://www.</description>
    </item>
    
    <item>
      <title>[Handson_ML]ch05: SVM</title>
      <link>http://taewan.kim/til/handson_ml_ch05/</link>
      <pubDate>Thu, 26 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/handson_ml_ch05/</guid>
      <description> SVM  적용 분야: 선형 분류, 비선형 분류, 회귀, 이상치 탐색 적용 데이터: 중/소 규모 데이터에 적합 복잡한 데이터를 잘 풀이
 용어
 Large Margin Classification: 라지 마진 분류 Support Vector: 도로 경계에 위치한 데이터  SVM은 도로 경계를 나누는 모델
 데이터의 Scale에 민감하게 만응 정규화를 선행해야 함 레이블을 반환, 확률을 반환하지 않음  SVM의 유형
 Hard Margin 분류: 하드 마진 분류  선형 분류가 가능해야 함 이상치에 만김함 일반화 성능 떨어짐  Soft Margin 분류: 소프트 마진 분류  마진 오류(Margin Violation)과 Margin의 균형 감 sklearn에서는 C 파라미터로 균형정도 설정  $C=\frac{1}{\lamba}$      선형 SVM  sklearn 클래스  LinearSVC(C=1, loss=&amp;lsquo;hinge&amp;rsquo;) SVC(kernel=&amp;ldquo;linear&amp;rdquo;, C=1)  데이터 셋이 커지면 속도가 느림  SGCClassfier(loss=hinge, alpha=1/(m*c))  경사 하강법 사용 속도는 LinearSVC보다 느리지만 대용량 데이터에 대하여 효과적으로 대응    비선형 SVM </description>
    </item>
    
    <item>
      <title>[til]문일천 교수님 기계학습 개론: 1week-Lec01 </title>
      <link>http://taewan.kim/til/ml_kooc_1week_01/</link>
      <pubDate>Thu, 26 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/ml_kooc_1week_01/</guid>
      <description>강좌 URL: http://kooc.kaist.ac.kr/machinelearning1_17  1주 Lecture 0: Introduction   강좌 동영상       최근 인공지능은 어디에서나 쓰이는 기술 요소  인공지능은 어디에서나 쓰이는 기술 요소 이메일 스팸필터 기능 SNS의 친구 추천 기능 동영상 재생 추천 기능 자동차 번호판 인식 기능 대규모 생산 공정에서의 품질 관리 적재적소에 물품을 운송하는 물류 시스템 군대의 자동화 무기 재난 대응을 위한 로봇    지능이란?
 지속적인 경험 축적을 통해서 어떠한 행동 및 의사 결정을 점점 더 잘 할 수 있다면 이러한 학습을 하는 대상은 지능이 있다고 말할 수 있음.</description>
    </item>
    
    <item>
      <title>[Coursera]Neural network and deep Learning: Week2</title>
      <link>http://taewan.kim/til/deeplearning_s01_week02/</link>
      <pubDate>Mon, 09 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/deeplearning_s01_week02/</guid>
      <description> Coursera에서 deeplearning.ai가 운영하는 &amp;lsquo;Neural network and deep learning ]&amp;lsquo;의 2주차 강의 정리입니다.
 강의 구성  Logistic Regression as a Neural Network  Binary Classification Logistic Regression Logistic Regression Cost Function Gradient Descent Derivatives More Derivative Examples Computation Graph Derivative with a computation graph Logistic Regression Gradient Descent Gradient Descent m Examples  Python and Vectorization  Vectorization More Vectorization Examples Vectorizing Logistic Regression Vectorizing Logistic Regression&amp;rsquo;s Gradient Output Broadcasting in Python A note on python/numpy vectors Quick tour of jupyter notebooks Explanation of logistic regression cost function    Logistic Regression as a Neural Network  for문을 지향하고 Vectorization을 수행 forward propagation과 back propagation을 소개  Binary Classification  Logistic regression은  </description>
    </item>
    
    <item>
      <title>[Coursera]Neural network and deep Learning: Week1</title>
      <link>http://taewan.kim/til/deeplearning_s01_week01/</link>
      <pubDate>Sun, 08 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/deeplearning_s01_week01/</guid>
      <description>Coursera에서 deeplearning.ai가 운영하는 &amp;lsquo;Neural network and deep learning ]&amp;lsquo;의 1주차 강의 정리입니다.
 동영상 목록  Welcome: (5min) What is a neural network (7min) Surpervised Learning with neural network (8min) Why is deep learning taking off? (10min) About This Courses? (2min) Frequently asked questions? (10min)   Welcome(5min)  AI is the new electircity 100년 전부터 전기 공급이 모든 산업계를 변화 시켰던 것 처럼, AI도 모든 산업계를 변화 시키고 있다.</description>
    </item>
    
    <item>
      <title>[20180707]&#39;파이썬 라이브러리를 활용한 머신러닝&#39; 1장</title>
      <link>http://taewan.kim/til/plwml-01/</link>
      <pubDate>Sat, 07 Jul 2018 21:28:14 +0900</pubDate>
      
      <guid>http://taewan.kim/til/plwml-01/</guid>
      <description>1장에서는 머신러닝의 기본 개념과 Scikit-learn에 대한 간략한 소개로 시작합니다. 또한 이 책에서 다룰 주요 환경에 대해 소개합니다. 마지막으로 KNN으로 붓꽃을 분류하는 예제를 통해서 데이터 수집, 적재, 탐색 및 학습의 과정을 소개합니다. 머신러닝의 가장 기본적인 용어와 접근법에 대한 기초적인 이해를 전달하는 것을 목표로합니다.
1장의 예제 코드와 실행 결과는 다음 URL에서 확인할 수 있습니다. - https://github.com/taewanme/notebooks4til/blob/master/MLWithPythonLibraries/ch01.ipynb
1장 구성 전에 26페이지 정도 분량으로 기계학습이란 무엇이고 이 책에서 다루는 환경에 대하여 소개합니다. 붓꽃을 분류하는 첫 번째 예제로 간단한 지도학습을 진행하는 방식을 소개합니다.</description>
    </item>
    
  </channel>
</rss>